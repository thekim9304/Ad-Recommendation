모델 1: 로지스틱 회귀 (Logistic Regression)

선형 결합을 시그모이드로 확률화하는 기본 분류기.

$\hat{y}=\sigma(w_1x_1 + \cdots + w_nx_n)$

핵심 포인트
- 범주형 피처: 원-핫 인코딩 필요(ad_category=의류 → ad_category_의류=1).
- 희소성 친화적: 원-핫 이후의 대규모 희소 행렬을 효율적으로 처리.
- 해석 가능: 피처별 기여(가중치) 해석 용이.
- 온라인 학습: FTRL 등으로 스트리밍 업데이트 가능.

한계
- 상호작용 부재: 피처 교차를 수작업으로 만들지 않으면 고차 상호작용 학습 불가.
- 비선형 패턴: 복잡한 비선형 관계 포착 한계.

⸻

모델 2: Factorization Machines (FM)

로지스틱 회귀의 한계(상호작용 미학습)를 잠재 벡터 내적으로 보완.

$\hat{y}= w_0+\sum_i w_ix_i+\sum_{i<j}\langle v_i, v_j\rangle x_ix_j$

핵심 포인트
- 잠재 벡터: 각 피처에 k차원 임베딩 v_i를 학습, 2차 상호작용을 내적으로 모델링.
- 희소/저빈도 일반화: 관측되지 않은 조합도 유사 잠재요인으로 추론 가능.
- 효율성: factorization trick으로 대규모에서도 계산량 제어.

한계
- 주로 2차 상호작용에 강점(그 이상은 별도 확장 필요).
- 임베딩 차원, 정규화 등 하이퍼파라미터 민감.

⸻

모델 3: GBDT (LightGBM/XGBoost)

다수의 트리를 부스팅으로 앙상블. 트리 분기로 비선형/상호작용 자동 학습.

핵심 포인트
- 자동 상호작용: 트리 깊이가 증가하며 고차 교차를 자연스럽게 포착.
- 실무 강자: 정형 데이터에서 강력한 베이스라인/프로덕션 모델.
- LightGBM 최적화: 히스토그램, 희소 최적화로 대규모 학습 가속.

한계
- 온라인 학습 어려움: 배치 재학습이 일반적.
- 초고유값 피처(예: user_id): 빈도 인코딩, 타 모델 출력 스태킹 등 보조 전처리 필요.

⸻

모델 4: Wide & Deep

암기(Memorization)와 일반화(Generalization)를 결합.

구성
- Wide: 수작업 교차 피처 + 선형(= 로지스틱 회귀) → 규칙/암기.
- Deep: 임베딩 + DNN → 비선형/일반화.
- 최종 출력은 Wide + Deep의 결합을 시그모이드로 확률화.

장점
- 고정 규칙(자주 등장하는 패턴)과 잠재 패턴(희소/신규 조합)을 동시에 커버.
- 대규모 범주형을 임베딩으로 처리.

한계
- Wide 측 교차 피처 설계가 필요(피처 공학 비용).

⸻

모델 5: DeepFM

Wide & Deep의 Wide(수작업 교차)를 FM 레이어로 대체하고, 임베딩을 공유해 End-to-End 학습.

구성
- FM 컴포넌트: 공유 임베딩으로 2차 상호작용 자동 학습.
- Deep 컴포넌트: 동일 임베딩을 DNN에 입력, 고차 상호작용 학습.
- 임베딩 공유: 두 컴포넌트가 같은 임베딩을 사용 → 표현력/효율성 ↑.

장점
- 수작업 교차 없이도 Wide 효과 대체.
- 대규모/희소 범주형에 적합, 구현 단순.

⸻

모델 6: DIN (Deep Interest Network)

어텐션으로 “현재 광고와 관련된 사용자 과거 행동”을 동적으로 강조.

핵심 아이디어
- 사용자 과거 행동 시퀀스 중, 현재 후보 광고와 유사한 행동에 높은 어텐션 가중치 부여.
- 가중 합으로 상황 의존적 사용자 관심 벡터 생성 → 후보 광고와 결합해 예측.

장점
- 고정 사용자 벡터 대신 광고-상황별 동적 관심사 반영.
- 클릭과 주제 유사성의 정합성을 강화.

한계
- 시퀀스 길이/어텐션 계산 비용 관리 필요.
- 피처/리스트 품질(노이즈) 민감.

⸻

모델 7: 최신 트렌드 (개요)
- DIEN: DIN + GRU로 관심사 진화(순서/시간)를 모델링.
- DSIN: 세션 단위 관심사 추출 + 세션 간 관계 학습.
- BST: RNN 대신 Transformer로 행동 시퀀스 모델링.
- MMoE: 멀티태스크(CTR, CVR 등)를 전문가 혼합으로 동시 학습, 서빙 효율/일관성 개선.

⸻

실무 메모
- 손실함수: BCE 기본, 클래스 가중치/네거티브 다운샘플링, 필요 시 Focal Loss(γ, α) 비교.
- 캘리브레이션: Platt/Isotonic으로 확률 보정(입찰/경매에 중요).
- 샘플링: 층화된 네거티브 다운샘플링으로 대표성+불균형 동시 처리.
- 지표: AUC/Logloss/PR-AUC + 랭킹(nDCG) + Calibration(ECE/ACE).
- 서빙: 실시간 제약(지연/캐시), 신규 유저·광고는 OOV/평균 임베딩 + 주기적 재학습.